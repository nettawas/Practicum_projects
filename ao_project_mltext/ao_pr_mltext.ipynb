{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Импорт основных библиотек\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Импорт библиотек для работы с текстами и NLP\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Импорт инструментов машинного обучения\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Импорт векторизаторов текста\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Загрузка ресурсов NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Инициализация стоп-слов\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Настройка отображения данных\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_2bb5e_row6_col0,#T_2bb5e_row6_col1,#T_2bb5e_row6_col2{\n",
       "            background-color:  lightcoral;\n",
       "        }</style><table id=\"T_2bb5e_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Unnamed: 0</th>        <th class=\"col_heading level0 col1\" >text</th>        <th class=\"col_heading level0 col2\" >toxic</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_2bb5e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_2bb5e_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "                        <td id=\"T_2bb5e_row0_col1\" class=\"data row0 col1\" >Explanation\n",
       "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "                        <td id=\"T_2bb5e_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2bb5e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_2bb5e_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "                        <td id=\"T_2bb5e_row1_col1\" class=\"data row1 col1\" >D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "                        <td id=\"T_2bb5e_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2bb5e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_2bb5e_row2_col0\" class=\"data row2 col0\" >2</td>\n",
       "                        <td id=\"T_2bb5e_row2_col1\" class=\"data row2 col1\" >Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "                        <td id=\"T_2bb5e_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2bb5e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_2bb5e_row3_col0\" class=\"data row3 col0\" >3</td>\n",
       "                        <td id=\"T_2bb5e_row3_col1\" class=\"data row3 col1\" >\"\n",
       "More\n",
       "I can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\n",
       "\n",
       "There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
       "                        <td id=\"T_2bb5e_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2bb5e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_2bb5e_row4_col0\" class=\"data row4 col0\" >4</td>\n",
       "                        <td id=\"T_2bb5e_row4_col1\" class=\"data row4 col1\" >You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "                        <td id=\"T_2bb5e_row4_col2\" class=\"data row4 col2\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2bb5e_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_2bb5e_row5_col0\" class=\"data row5 col0\" >5</td>\n",
       "                        <td id=\"T_2bb5e_row5_col1\" class=\"data row5 col1\" >\"\n",
       "\n",
       "Congratulations from me as well, use the tools well.  · talk \"</td>\n",
       "                        <td id=\"T_2bb5e_row5_col2\" class=\"data row5 col2\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2bb5e_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_2bb5e_row6_col0\" class=\"data row6 col0\" >6</td>\n",
       "                        <td id=\"T_2bb5e_row6_col1\" class=\"data row6 col1\" >COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "                        <td id=\"T_2bb5e_row6_col2\" class=\"data row6 col2\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2bb5e_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_2bb5e_row7_col0\" class=\"data row7 col0\" >7</td>\n",
       "                        <td id=\"T_2bb5e_row7_col1\" class=\"data row7 col1\" >Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.</td>\n",
       "                        <td id=\"T_2bb5e_row7_col2\" class=\"data row7 col2\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2bb5e_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_2bb5e_row8_col0\" class=\"data row8 col0\" >8</td>\n",
       "                        <td id=\"T_2bb5e_row8_col1\" class=\"data row8 col1\" >Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169</td>\n",
       "                        <td id=\"T_2bb5e_row8_col2\" class=\"data row8 col2\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2bb5e_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_2bb5e_row9_col0\" class=\"data row9 col0\" >9</td>\n",
       "                        <td id=\"T_2bb5e_row9_col1\" class=\"data row9 col1\" >alignment on this subject and which are contrary to those of DuLithgow</td>\n",
       "                        <td id=\"T_2bb5e_row9_col2\" class=\"data row9 col2\" >0</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7efccbe8c580>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Загрузка данных из CSV-файла\n",
    "df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "\n",
    "# Подсветим токсичные комментарии\n",
    "def highlight_toxic(row):\n",
    "    color = 'background-color: lightcoral' if row['toxic'] == 1 else ''\n",
    "    return [color] * len(row)\n",
    "\n",
    "# Применим стиль и выведем первые строки\n",
    "display(df.head(10).style.apply(highlight_toxic, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер таблицы: (159292, 3)\n"
     ]
    }
   ],
   "source": [
    "# Проверим размер таблицы\n",
    "print(f\"Размер таблицы: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Вывод информации о данных\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас 159 292 комментария в датасете.\n",
    "\n",
    "\n",
    "Столбец text содержит тексты комментариев.\n",
    "\n",
    "\n",
    "Столбец toxic содержит метку (0 — нормальный комментарий, 1 — токсичный).\n",
    "\n",
    "\n",
    "Пропусков в данных нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество дубликатов: 0\n"
     ]
    }
   ],
   "source": [
    "# Проверка количества дубликатов\n",
    "print(f\"Количество дубликатов: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"\n",
    "    Переводит POS-тег (NN, VB, JJ и т.д.) из NLTK\n",
    "    в формат, который понимает WordNetLemmatizer (wordnet.NOUN и т.п.).\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentClassifier:\n",
    "    \"\"\"Класс для предобработки текстов и классификации комментариев.\"\"\"\n",
    "\n",
    "    def __init__(self, models_and_params, score, solvers, stop_words, df, target_column, text_column):\n",
    "        \"\"\"\n",
    "        Инициализация класса.\n",
    "        \n",
    "        Аргументы:\n",
    "        - models_and_params: список кортежей (модель, параметры для GridSearchCV)\n",
    "        - score: метрика для оценки\n",
    "        - solvers: список векторизаторов (например, [TfidfVectorizer, CountVectorizer])\n",
    "        - stop_words: список стоп-слов\n",
    "        - df: DataFrame с данными\n",
    "        - target_column: название целевого столбца\n",
    "        - text_column: название столбца с текстом\n",
    "        \"\"\"\n",
    "        self.models_and_params = models_and_params\n",
    "        self.score = score\n",
    "        self.solvers = solvers\n",
    "        self.stop_words = stop_words\n",
    "        self.df = df\n",
    "        self.target_column = target_column\n",
    "        self.text_column = text_column\n",
    "\n",
    "        # Проверка наличия нужных столбцов\n",
    "        if target_column not in df.columns or text_column not in df.columns:\n",
    "            raise ValueError(\"Ошибка: Указанные столбцы не найдены в DataFrame!\")\n",
    "\n",
    "        # Переменные для хранения данных\n",
    "        self.best_model = None\n",
    "        self.max_score = -1\n",
    "        self.results = {}\n",
    "\n",
    "        # Запуск этапов предобработки\n",
    "        self._clean_text()\n",
    "        print(\"✅ Текст очищен.\")\n",
    "        self._lemmatize()\n",
    "        print(\"✅ Лемматизация завершена.\")\n",
    "        self._split_data()\n",
    "        print(\"✅ Данные разделены на train/val/test.\")\n",
    "\n",
    "    def _clean_text(self):\n",
    "        \"\"\"Очищает текст от лишних символов.\"\"\"\n",
    "        self.df[self.text_column] = self.df[self.text_column].astype(str).str.lower()\n",
    "        self.df[self.text_column] = self.df[self.text_column].apply(lambda x: re.sub(r'[^a-z\\s]', '', x))\n",
    "        self.df[self.text_column] = self.df[self.text_column].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "\n",
    "    def _lemmatize(self):\n",
    "        \"\"\"Лемматизирует текст с учётом частей речи (POS-тегов).\"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        def lemmatize_text(text):\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            pos_tags = nltk.pos_tag(tokens)\n",
    "            return \" \".join([\n",
    "                lemmatizer.lemmatize(token, get_wordnet_pos(pos))\n",
    "                for token, pos in pos_tags\n",
    "            ])\n",
    "        self.df[self.text_column] = self.df[self.text_column].apply(lemmatize_text)\n",
    "\n",
    "    def _split_data(self):\n",
    "        \"\"\"Разделяет данные на train, validation и test.\"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.df[self.text_column], \n",
    "            self.df[self.target_column], \n",
    "            test_size=0.2, \n",
    "            random_state=42\n",
    "        )\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "        self.splits = {\n",
    "            'train': (X_train, y_train),\n",
    "            'val': (X_val, y_val),\n",
    "            'test': (X_test, y_test)\n",
    "        }\n",
    "\n",
    "\n",
    "    def train_models(self):\n",
    "        \"\"\"Обучает модели с использованием Pipeline, чтобы избежать утечки данных.\"\"\"\n",
    "        self.results = {}\n",
    "\n",
    "        for model, params in tqdm(self.models_and_params, desc=\"Обучение моделей\"):\n",
    "            for vectorizer in self.solvers:\n",
    "                \n",
    "                pipe = Pipeline([\n",
    "                    ('vect', vectorizer(stop_words=self.stop_words)),\n",
    "                    ('clf', model)\n",
    "                ])\n",
    "            \n",
    "            \n",
    "                param_distributions = {f'clf__{key}': val for key, val in params.items()}\n",
    "\n",
    "            \n",
    "                rand = RandomizedSearchCV(\n",
    "                    estimator=pipe,\n",
    "                    param_distributions=param_distributions,\n",
    "                    n_iter=2,              \n",
    "                    cv=3,\n",
    "                    scoring=self.score,\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42\n",
    "                )\n",
    "\n",
    "                X_train, y_train = self.splits['train']\n",
    "                X_val, y_val = self.splits['val']\n",
    "                X_test, y_test = self.splits['test']\n",
    "\n",
    "                rand.fit(X_train, y_train)\n",
    "\n",
    "                best_model = rand.best_estimator_\n",
    "                best_score_train = rand.best_score_\n",
    "                best_score_val = f1_score(y_val, best_model.predict(X_val))\n",
    "                best_score_test = f1_score(y_test, best_model.predict(X_test))\n",
    "\n",
    "                name = f\"{model.__class__.__name__} + {vectorizer.__name__}\"\n",
    "                self.results[name] = {\n",
    "                    \"best_model\": best_model,\n",
    "                    \"train_score\": best_score_train,\n",
    "                    \"val_score\": best_score_val,\n",
    "                    \"test_score\": best_score_test\n",
    "                }\n",
    "\n",
    "                if best_score_val > self.max_score:\n",
    "                    self.max_score = best_score_test\n",
    "                    self.best_model = best_model\n",
    "\n",
    "        return {\"max_score\": self.max_score, \"best_model\": self.best_model}\n",
    "\n",
    "\n",
    "    def get_results(self):\n",
    "        \"\"\"Возвращает словарь с результатами обучения.\"\"\"\n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Берём 20% данных\n",
    "df_small = df.sample(frac=0.2, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Гиперпараметры для моделей\n",
    "params_Log = {\n",
    "    \"max_iter\": [1000],\n",
    "    \"C\": [1, 5]\n",
    "}\n",
    "\n",
    "params_RF = {\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Текст очищен.\n",
      "✅ Лемматизация завершена.\n",
      "✅ Данные разделены на train/val/test.\n"
     ]
    }
   ],
   "source": [
    "# Создаём объект классификатора\n",
    "\n",
    "cl_small = ToxicCommentClassifier(\n",
    "    models_and_params=[\n",
    "        (LogisticRegression(random_state=42, class_weight='balanced', n_jobs=-1), params_Log),\n",
    "        (RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1), params_RF)\n",
    "    ],\n",
    "    score='f1',\n",
    "    solvers=[TfidfVectorizer, CountVectorizer],\n",
    "    stop_words=stop_words,\n",
    "    df=df_small,\n",
    "    target_column='toxic',\n",
    "    text_column='text'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31055</td>\n",
       "      <td>sometime back i just happen to log on to wwwizoomin with a friend reference and i be amaze to see the concept fresh idea entertainment have come up with so many deal all under one roof this website be very user friendly and easy to use and be fun to be on you have gossip game facts another exciting feature to add to it be face of the week every week new face be select and put up a izoom face it great to have be select in four out of a group of million this new start up have already get many a deal in it kitty few of them be thefortune hotel the beach be my personal favorite izoomin have a usp of mobile coupon coupon be available even when a user can not access internet you just need to sms izoom support to and you get attend immediately all i can say be izoomin be a must visit website for everyone before they go out for shopping or dining or for out cheer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102929</td>\n",
       "      <td>the late edit be much well dont make this article state super at all</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67385</td>\n",
       "      <td>october utc i would think youd be able to get your point across and be immune to any objection be you to simply embellish the second sentence of the article by change he be school at thornleigh salesian college to he be school at the then allcatholic thornleigh salesian college good suggestion from an anon what do you think rgds</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81167</td>\n",
       "      <td>thanks for the tip on the currency translation think it all do now</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90182</td>\n",
       "      <td>i would argue that if content on the con in comparison to the art music be out of proportion then it warrant far contribution to the article not the removal of an indepth piece of content also a i mention before the art music unit have a notable history comparable to that of the con itself because of this i would far argue that content on the art music unit be more relevant to this article than the information on the newcastle conservatorium</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0       31055   \n",
       "1      102929   \n",
       "2       67385   \n",
       "3       81167   \n",
       "4       90182   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "0  sometime back i just happen to log on to wwwizoomin with a friend reference and i be amaze to see the concept fresh idea entertainment have come up with so many deal all under one roof this website be very user friendly and easy to use and be fun to be on you have gossip game facts another exciting feature to add to it be face of the week every week new face be select and put up a izoom face it great to have be select in four out of a group of million this new start up have already get many a deal in it kitty few of them be thefortune hotel the beach be my personal favorite izoomin have a usp of mobile coupon coupon be available even when a user can not access internet you just need to sms izoom support to and you get attend immediately all i can say be izoomin be a must visit website for everyone before they go out for shopping or dining or for out cheer   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 the late edit be much well dont make this article state super at all   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           october utc i would think youd be able to get your point across and be immune to any objection be you to simply embellish the second sentence of the article by change he be school at thornleigh salesian college to he be school at the then allcatholic thornleigh salesian college good suggestion from an anon what do you think rgds   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   thanks for the tip on the currency translation think it all do now   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                        i would argue that if content on the con in comparison to the art music be out of proportion then it warrant far contribution to the article not the removal of an indepth piece of content also a i mention before the art music unit have a notable history comparable to that of the con itself because of this i would far argue that content on the art music unit be more relevant to this article than the information on the newcastle conservatorium   \n",
       "\n",
       "   toxic  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_small.df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обучение моделей:  50%|█████     | 1/2 [07:21<07:21, 441.73s/it]/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=2. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=2. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "Обучение моделей: 100%|██████████| 2/2 [07:32<00:00, 226.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты обучения на 20% данных: {'max_score': 0.7503828483920368, 'best_model': Pipeline(steps=[('vect',\n",
      "                 CountVectorizer(stop_words=['a', 'about', 'above', 'after',\n",
      "                                             'again', 'against', 'ain', 'all',\n",
      "                                             'am', 'an', 'and', 'any', 'are',\n",
      "                                             'aren', \"aren't\", 'as', 'at', 'be',\n",
      "                                             'because', 'been', 'before',\n",
      "                                             'being', 'below', 'between',\n",
      "                                             'both', 'but', 'by', 'can',\n",
      "                                             'couldn', \"couldn't\", ...])),\n",
      "                ('clf',\n",
      "                 LogisticRegression(C=5, class_weight='balanced', max_iter=1000,\n",
      "                                    n_jobs=-1, random_state=42))])}\n",
      "Лучшая модель: Pipeline(steps=[('vect',\n",
      "                 CountVectorizer(stop_words=['a', 'about', 'above', 'after',\n",
      "                                             'again', 'against', 'ain', 'all',\n",
      "                                             'am', 'an', 'and', 'any', 'are',\n",
      "                                             'aren', \"aren't\", 'as', 'at', 'be',\n",
      "                                             'because', 'been', 'before',\n",
      "                                             'being', 'below', 'between',\n",
      "                                             'both', 'but', 'by', 'can',\n",
      "                                             'couldn', \"couldn't\", ...])),\n",
      "                ('clf',\n",
      "                 LogisticRegression(C=5, class_weight='balanced', max_iter=1000,\n",
      "                                    n_jobs=-1, random_state=42))])\n",
      "Лучший тестовый F1: 0.7503828483920368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Запуск обучения\n",
    "results_small = cl_small.train_models()\n",
    "\n",
    "# Выводим результаты\n",
    "print(\"Результаты обучения на 20% данных:\", results_small)\n",
    "print(\"Лучшая модель:\", cl_small.best_model)\n",
    "print(\"Лучший тестовый F1:\", cl_small.max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. По результатам обучения на 20% данных:\n",
    "\n",
    "\n",
    "- Лучшая модель: LogisticRegression(class_weight='balanced', max_iter=1000, n_jobs=-1, random_state=42)\n",
    "\n",
    "\n",
    "- Лучшая метрика F1: ≈ 0.75\n",
    "\n",
    "\n",
    "2. Поскольку модель достигла F1 выше 0.75 даже на уменьшенной выборке, можно сделать вывод, что:\n",
    "\n",
    "\n",
    "- Логистическая регрессия с учётом баланса классов неплохо справляется с задачей определения токсичности комментариев.\n",
    "\n",
    "\n",
    "- Для ускорения расчётов мы использовали 20% данных и сократили сетку гиперпараметров; при обучении на полном датасете модель, вероятно, может показать ещё более стабильный результат, но потребует больше времени.\n",
    "\n",
    "\n",
    "Таким образом, проект успешно демонстрирует построение модели классификации токсичных комментариев с требуемым уровнем качества F1 ≥ 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [x]  Весь код выполняется без ошибок\n",
    "- [x]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [x]  Данные загружены и подготовлены\n",
    "- [x]  Модели обучены\n",
    "- [x]  Значение метрики *F1* не меньше 0.75\n",
    "- [x]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 2423,
    "start_time": "2025-03-21T10:25:25.015Z"
   },
   {
    "duration": 143,
    "start_time": "2025-03-21T10:26:13.521Z"
   },
   {
    "duration": 54,
    "start_time": "2025-03-21T10:26:52.079Z"
   },
   {
    "duration": 62,
    "start_time": "2025-03-21T10:27:19.719Z"
   },
   {
    "duration": 17,
    "start_time": "2025-03-21T10:27:49.181Z"
   },
   {
    "duration": 17,
    "start_time": "2025-03-21T10:28:04.902Z"
   },
   {
    "duration": 15,
    "start_time": "2025-03-21T10:28:31.863Z"
   },
   {
    "duration": 5,
    "start_time": "2025-03-21T10:29:05.068Z"
   },
   {
    "duration": 63,
    "start_time": "2025-03-21T10:29:25.810Z"
   },
   {
    "duration": 56,
    "start_time": "2025-03-21T10:29:44.419Z"
   },
   {
    "duration": 1028,
    "start_time": "2025-03-21T10:30:38.858Z"
   },
   {
    "duration": 981,
    "start_time": "2025-03-21T10:31:09.170Z"
   },
   {
    "duration": 1033,
    "start_time": "2025-03-21T10:31:14.297Z"
   },
   {
    "duration": 945,
    "start_time": "2025-03-21T10:31:21.581Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-21T10:31:26.805Z"
   },
   {
    "duration": 33,
    "start_time": "2025-03-21T10:32:58.181Z"
   },
   {
    "duration": 163,
    "start_time": "2025-03-21T10:33:18.667Z"
   },
   {
    "duration": 974,
    "start_time": "2025-03-21T10:34:39.936Z"
   },
   {
    "duration": 251,
    "start_time": "2025-03-21T10:37:20.752Z"
   },
   {
    "duration": 207,
    "start_time": "2025-03-21T10:38:32.089Z"
   },
   {
    "duration": 2024,
    "start_time": "2025-03-21T10:40:24.824Z"
   },
   {
    "duration": 1268,
    "start_time": "2025-03-21T10:40:26.851Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-21T10:40:28.120Z"
   },
   {
    "duration": 58,
    "start_time": "2025-03-21T10:40:28.126Z"
   },
   {
    "duration": 290,
    "start_time": "2025-03-21T10:40:28.186Z"
   },
   {
    "duration": 22,
    "start_time": "2025-03-21T10:42:08.870Z"
   },
   {
    "duration": 5,
    "start_time": "2025-03-21T10:52:28.057Z"
   },
   {
    "duration": 113166,
    "start_time": "2025-03-21T10:52:46.986Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-21T10:56:18.812Z"
   },
   {
    "duration": 141,
    "start_time": "2025-03-21T10:56:55.007Z"
   },
   {
    "duration": 2181,
    "start_time": "2025-03-21T10:57:05.447Z"
   },
   {
    "duration": 1201,
    "start_time": "2025-03-21T10:57:07.631Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-21T10:57:08.833Z"
   },
   {
    "duration": 95,
    "start_time": "2025-03-21T10:57:08.838Z"
   },
   {
    "duration": 264,
    "start_time": "2025-03-21T10:57:08.935Z"
   },
   {
    "duration": 18,
    "start_time": "2025-03-21T10:57:09.201Z"
   },
   {
    "duration": 5,
    "start_time": "2025-03-21T10:57:09.221Z"
   },
   {
    "duration": 114618,
    "start_time": "2025-03-21T10:57:09.228Z"
   },
   {
    "duration": 162,
    "start_time": "2025-03-21T10:59:03.849Z"
   },
   {
    "duration": 1875,
    "start_time": "2025-03-21T11:07:12.629Z"
   },
   {
    "duration": 1203,
    "start_time": "2025-03-21T11:07:14.506Z"
   },
   {
    "duration": 3,
    "start_time": "2025-03-21T11:07:15.711Z"
   },
   {
    "duration": 57,
    "start_time": "2025-03-21T11:07:15.716Z"
   },
   {
    "duration": 253,
    "start_time": "2025-03-21T11:07:15.777Z"
   },
   {
    "duration": 31,
    "start_time": "2025-03-21T11:07:16.032Z"
   },
   {
    "duration": 11,
    "start_time": "2025-03-21T11:07:16.065Z"
   },
   {
    "duration": 110498,
    "start_time": "2025-03-21T11:07:16.077Z"
   },
   {
    "duration": 2251,
    "start_time": "2025-03-24T07:18:21.813Z"
   },
   {
    "duration": 1106,
    "start_time": "2025-03-24T07:18:24.071Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-24T07:18:25.179Z"
   },
   {
    "duration": 35,
    "start_time": "2025-03-24T07:18:25.184Z"
   },
   {
    "duration": 261,
    "start_time": "2025-03-24T07:18:25.221Z"
   },
   {
    "duration": 16,
    "start_time": "2025-03-24T07:18:25.484Z"
   },
   {
    "duration": 10,
    "start_time": "2025-03-24T07:18:25.501Z"
   },
   {
    "duration": 109107,
    "start_time": "2025-03-24T07:18:25.512Z"
   },
   {
    "duration": 3,
    "start_time": "2025-03-24T07:20:14.620Z"
   },
   {
    "duration": 105435,
    "start_time": "2025-03-24T07:21:38.261Z"
   },
   {
    "duration": 94746,
    "start_time": "2025-03-24T07:31:19.785Z"
   },
   {
    "duration": 2164,
    "start_time": "2025-04-07T13:47:22.452Z"
   },
   {
    "duration": 1628,
    "start_time": "2025-04-07T13:47:24.619Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-07T13:47:26.250Z"
   },
   {
    "duration": 68,
    "start_time": "2025-04-07T13:47:26.257Z"
   },
   {
    "duration": 261,
    "start_time": "2025-04-07T13:47:26.327Z"
   },
   {
    "duration": 20,
    "start_time": "2025-04-07T13:47:26.590Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-07T13:47:26.617Z"
   },
   {
    "duration": 116274,
    "start_time": "2025-04-07T13:47:26.627Z"
   },
   {
    "duration": 78,
    "start_time": "2025-04-07T13:49:22.907Z"
   },
   {
    "duration": 17,
    "start_time": "2025-04-07T13:53:46.350Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-07T13:53:47.303Z"
   },
   {
    "duration": 107046,
    "start_time": "2025-04-07T13:53:47.962Z"
   },
   {
    "duration": 79145,
    "start_time": "2025-04-07T13:55:45.412Z"
   },
   {
    "duration": 11,
    "start_time": "2025-04-07T13:57:19.763Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-07T13:58:03.869Z"
   },
   {
    "duration": 102522,
    "start_time": "2025-04-07T13:58:04.585Z"
   },
   {
    "duration": 11,
    "start_time": "2025-04-07T14:01:28.099Z"
   },
   {
    "duration": 11,
    "start_time": "2025-04-07T14:01:33.856Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-07T14:04:34.877Z"
   },
   {
    "duration": 1877,
    "start_time": "2025-04-07T14:05:12.749Z"
   },
   {
    "duration": 1168,
    "start_time": "2025-04-07T14:05:14.629Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-07T14:05:15.807Z"
   },
   {
    "duration": 36,
    "start_time": "2025-04-07T14:05:15.813Z"
   },
   {
    "duration": 267,
    "start_time": "2025-04-07T14:05:15.851Z"
   },
   {
    "duration": 17,
    "start_time": "2025-04-07T14:05:16.121Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-07T14:05:16.140Z"
   },
   {
    "duration": 110367,
    "start_time": "2025-04-07T14:05:16.149Z"
   },
   {
    "duration": 142,
    "start_time": "2025-04-07T14:07:06.518Z"
   },
   {
    "duration": 17,
    "start_time": "2025-04-07T14:08:21.034Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-07T14:08:23.945Z"
   },
   {
    "duration": 106693,
    "start_time": "2025-04-07T14:08:25.084Z"
   },
   {
    "duration": 168107,
    "start_time": "2025-04-07T14:34:17.731Z"
   },
   {
    "duration": 21,
    "start_time": "2025-04-07T14:39:01.219Z"
   },
   {
    "duration": 32764,
    "start_time": "2025-04-07T14:39:18.108Z"
   },
   {
    "duration": 2189,
    "start_time": "2025-04-08T09:36:58.908Z"
   },
   {
    "duration": 1096,
    "start_time": "2025-04-08T09:37:01.099Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-08T09:37:02.197Z"
   },
   {
    "duration": 122,
    "start_time": "2025-04-08T09:37:02.201Z"
   },
   {
    "duration": 271,
    "start_time": "2025-04-08T09:37:02.325Z"
   },
   {
    "duration": 16,
    "start_time": "2025-04-08T09:37:02.598Z"
   },
   {
    "duration": 40,
    "start_time": "2025-04-08T09:37:02.616Z"
   },
   {
    "duration": 20671,
    "start_time": "2025-04-08T09:37:02.657Z"
   },
   {
    "duration": 227604,
    "start_time": "2025-04-08T09:37:23.329Z"
   },
   {
    "duration": 4221,
    "start_time": "2025-04-08T10:11:45.788Z"
   },
   {
    "duration": 1199,
    "start_time": "2025-04-08T10:11:50.011Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-08T10:11:51.212Z"
   },
   {
    "duration": 48,
    "start_time": "2025-04-08T10:11:51.216Z"
   },
   {
    "duration": 255,
    "start_time": "2025-04-08T10:11:51.267Z"
   },
   {
    "duration": 18,
    "start_time": "2025-04-08T10:11:51.524Z"
   },
   {
    "duration": 30,
    "start_time": "2025-04-08T10:11:51.544Z"
   },
   {
    "duration": 23683,
    "start_time": "2025-04-08T10:11:51.577Z"
   },
   {
    "duration": 143,
    "start_time": "2025-04-08T10:16:42.623Z"
   },
   {
    "duration": 12,
    "start_time": "2025-04-08T10:16:53.887Z"
   },
   {
    "duration": 11,
    "start_time": "2025-04-08T10:16:58.144Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-08T10:17:16.818Z"
   },
   {
    "duration": 11,
    "start_time": "2025-04-08T10:17:28.265Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-08T10:17:36.528Z"
   },
   {
    "duration": 2186,
    "start_time": "2025-04-09T12:41:16.771Z"
   },
   {
    "duration": 1067,
    "start_time": "2025-04-09T12:41:18.959Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-09T12:41:20.035Z"
   },
   {
    "duration": 35,
    "start_time": "2025-04-09T12:41:20.039Z"
   },
   {
    "duration": 249,
    "start_time": "2025-04-09T12:41:20.076Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-09T12:41:20.327Z"
   },
   {
    "duration": 45,
    "start_time": "2025-04-09T12:41:20.337Z"
   },
   {
    "duration": 16,
    "start_time": "2025-04-09T12:41:20.384Z"
   },
   {
    "duration": 1455,
    "start_time": "2025-04-09T12:41:20.401Z"
   },
   {
    "duration": 0,
    "start_time": "2025-04-09T12:41:21.858Z"
   },
   {
    "duration": 0,
    "start_time": "2025-04-09T12:41:21.859Z"
   },
   {
    "duration": 2034,
    "start_time": "2025-04-09T12:42:57.195Z"
   },
   {
    "duration": 1019,
    "start_time": "2025-04-09T12:42:59.231Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-09T12:43:00.252Z"
   },
   {
    "duration": 48,
    "start_time": "2025-04-09T12:43:00.256Z"
   },
   {
    "duration": 318,
    "start_time": "2025-04-09T12:43:00.306Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-09T12:43:00.627Z"
   },
   {
    "duration": 17,
    "start_time": "2025-04-09T12:43:00.638Z"
   },
   {
    "duration": 21,
    "start_time": "2025-04-09T12:43:00.657Z"
   },
   {
    "duration": 103457,
    "start_time": "2025-04-09T12:43:00.679Z"
   },
   {
    "duration": 12,
    "start_time": "2025-04-09T12:44:44.138Z"
   },
   {
    "duration": 228317,
    "start_time": "2025-04-09T12:44:44.152Z"
   },
   {
    "duration": 1825,
    "start_time": "2025-04-09T13:11:02.680Z"
   },
   {
    "duration": 1088,
    "start_time": "2025-04-09T13:11:04.507Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-09T13:11:05.597Z"
   },
   {
    "duration": 151,
    "start_time": "2025-04-09T13:11:05.602Z"
   },
   {
    "duration": 233,
    "start_time": "2025-04-09T13:11:05.755Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-09T13:11:05.989Z"
   },
   {
    "duration": 82,
    "start_time": "2025-04-09T13:11:05.995Z"
   },
   {
    "duration": 0,
    "start_time": "2025-04-09T13:11:06.079Z"
   },
   {
    "duration": 0,
    "start_time": "2025-04-09T13:11:06.080Z"
   },
   {
    "duration": 0,
    "start_time": "2025-04-09T13:11:06.082Z"
   },
   {
    "duration": 0,
    "start_time": "2025-04-09T13:11:06.083Z"
   },
   {
    "duration": 1764,
    "start_time": "2025-04-09T13:14:29.647Z"
   },
   {
    "duration": 1138,
    "start_time": "2025-04-09T13:14:31.413Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-09T13:14:32.554Z"
   },
   {
    "duration": 35,
    "start_time": "2025-04-09T13:14:32.560Z"
   },
   {
    "duration": 272,
    "start_time": "2025-04-09T13:14:32.598Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-09T13:14:32.872Z"
   },
   {
    "duration": 17,
    "start_time": "2025-04-09T13:14:32.879Z"
   },
   {
    "duration": 20,
    "start_time": "2025-04-09T13:14:32.897Z"
   },
   {
    "duration": 108739,
    "start_time": "2025-04-09T13:14:32.918Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-09T13:16:21.659Z"
   },
   {
    "duration": 288103,
    "start_time": "2025-04-09T13:16:21.669Z"
   },
   {
    "duration": 64,
    "start_time": "2025-04-09T13:21:10.796Z"
   },
   {
    "duration": 1825,
    "start_time": "2025-04-09T13:28:25.316Z"
   },
   {
    "duration": 1102,
    "start_time": "2025-04-09T13:28:27.148Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-09T13:28:28.252Z"
   },
   {
    "duration": 35,
    "start_time": "2025-04-09T13:28:28.256Z"
   },
   {
    "duration": 268,
    "start_time": "2025-04-09T13:28:28.293Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-09T13:28:28.562Z"
   },
   {
    "duration": 15,
    "start_time": "2025-04-09T13:28:28.568Z"
   },
   {
    "duration": 20,
    "start_time": "2025-04-09T13:28:28.585Z"
   },
   {
    "duration": 160488,
    "start_time": "2025-04-09T13:28:28.606Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-09T13:31:09.096Z"
   },
   {
    "duration": 342686,
    "start_time": "2025-04-09T13:31:09.106Z"
   },
   {
    "duration": 1744,
    "start_time": "2025-04-09T13:41:16.235Z"
   },
   {
    "duration": 1038,
    "start_time": "2025-04-09T13:41:17.981Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-09T13:41:19.020Z"
   },
   {
    "duration": 43,
    "start_time": "2025-04-09T13:41:19.024Z"
   },
   {
    "duration": 244,
    "start_time": "2025-04-09T13:41:19.070Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-09T13:41:19.316Z"
   },
   {
    "duration": 42,
    "start_time": "2025-04-09T13:41:19.322Z"
   },
   {
    "duration": 23,
    "start_time": "2025-04-09T13:41:19.366Z"
   },
   {
    "duration": 252251,
    "start_time": "2025-04-09T13:41:19.390Z"
   },
   {
    "duration": 10,
    "start_time": "2025-04-09T13:45:31.642Z"
   },
   {
    "duration": 1176488,
    "start_time": "2025-04-09T13:45:31.653Z"
   },
   {
    "duration": 1825,
    "start_time": "2025-04-09T14:06:11.753Z"
   },
   {
    "duration": 1074,
    "start_time": "2025-04-09T14:06:13.580Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-09T14:06:14.656Z"
   },
   {
    "duration": 34,
    "start_time": "2025-04-09T14:06:14.660Z"
   },
   {
    "duration": 260,
    "start_time": "2025-04-09T14:06:14.696Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-09T14:06:14.957Z"
   },
   {
    "duration": 15,
    "start_time": "2025-04-09T14:06:14.963Z"
   },
   {
    "duration": 26,
    "start_time": "2025-04-09T14:06:14.980Z"
   },
   {
    "duration": 158932,
    "start_time": "2025-04-09T14:06:15.007Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-09T14:08:53.940Z"
   },
   {
    "duration": 2505,
    "start_time": "2025-04-10T12:20:56.262Z"
   },
   {
    "duration": 1033,
    "start_time": "2025-04-10T12:20:58.769Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-10T12:20:59.804Z"
   },
   {
    "duration": 28,
    "start_time": "2025-04-10T12:20:59.809Z"
   },
   {
    "duration": 229,
    "start_time": "2025-04-10T12:20:59.839Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-10T12:21:00.070Z"
   },
   {
    "duration": 27,
    "start_time": "2025-04-10T12:21:00.076Z"
   },
   {
    "duration": 66,
    "start_time": "2025-04-10T12:21:00.104Z"
   },
   {
    "duration": 142211,
    "start_time": "2025-04-10T12:21:00.171Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-10T12:23:22.384Z"
   },
   {
    "duration": 130540,
    "start_time": "2025-04-10T12:23:22.395Z"
   },
   {
    "duration": 1682,
    "start_time": "2025-04-10T12:31:24.695Z"
   },
   {
    "duration": 929,
    "start_time": "2025-04-10T12:31:26.379Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-10T12:31:27.309Z"
   },
   {
    "duration": 32,
    "start_time": "2025-04-10T12:31:27.313Z"
   },
   {
    "duration": 224,
    "start_time": "2025-04-10T12:31:27.347Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-10T12:31:27.573Z"
   },
   {
    "duration": 12,
    "start_time": "2025-04-10T12:31:27.578Z"
   },
   {
    "duration": 21,
    "start_time": "2025-04-10T12:31:27.592Z"
   },
   {
    "duration": 136523,
    "start_time": "2025-04-10T12:31:27.614Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-10T12:33:44.138Z"
   },
   {
    "duration": 1615,
    "start_time": "2025-04-10T12:47:54.349Z"
   },
   {
    "duration": 955,
    "start_time": "2025-04-10T12:47:55.966Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-10T12:47:56.923Z"
   },
   {
    "duration": 29,
    "start_time": "2025-04-10T12:47:56.927Z"
   },
   {
    "duration": 212,
    "start_time": "2025-04-10T12:47:56.971Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-10T12:47:57.185Z"
   },
   {
    "duration": 105,
    "start_time": "2025-04-10T12:47:57.190Z"
   },
   {
    "duration": 0,
    "start_time": "2025-04-10T12:47:57.297Z"
   },
   {
    "duration": 0,
    "start_time": "2025-04-10T12:47:57.298Z"
   },
   {
    "duration": 0,
    "start_time": "2025-04-10T12:47:57.299Z"
   },
   {
    "duration": 0,
    "start_time": "2025-04-10T12:47:57.301Z"
   },
   {
    "duration": 1657,
    "start_time": "2025-04-10T12:50:57.036Z"
   },
   {
    "duration": 977,
    "start_time": "2025-04-10T12:50:58.695Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-10T12:50:59.674Z"
   },
   {
    "duration": 31,
    "start_time": "2025-04-10T12:50:59.679Z"
   },
   {
    "duration": 233,
    "start_time": "2025-04-10T12:50:59.713Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-10T12:50:59.948Z"
   },
   {
    "duration": 28,
    "start_time": "2025-04-10T12:50:59.954Z"
   },
   {
    "duration": 16,
    "start_time": "2025-04-10T12:50:59.984Z"
   },
   {
    "duration": 92861,
    "start_time": "2025-04-10T12:51:00.002Z"
   },
   {
    "duration": 11,
    "start_time": "2025-04-10T12:52:32.865Z"
   },
   {
    "duration": 424309,
    "start_time": "2025-04-10T12:52:32.877Z"
   },
   {
    "duration": 1729,
    "start_time": "2025-04-10T13:02:46.305Z"
   },
   {
    "duration": 1006,
    "start_time": "2025-04-10T13:02:48.036Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-10T13:02:49.043Z"
   },
   {
    "duration": 45,
    "start_time": "2025-04-10T13:02:49.047Z"
   },
   {
    "duration": 229,
    "start_time": "2025-04-10T13:02:49.094Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-10T13:02:49.324Z"
   },
   {
    "duration": 13,
    "start_time": "2025-04-10T13:02:49.329Z"
   },
   {
    "duration": 26,
    "start_time": "2025-04-10T13:02:49.344Z"
   },
   {
    "duration": 94678,
    "start_time": "2025-04-10T13:02:49.372Z"
   },
   {
    "duration": 19,
    "start_time": "2025-04-10T13:04:24.052Z"
   },
   {
    "duration": 452951,
    "start_time": "2025-04-10T13:04:24.072Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
